{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdb76af",
   "metadata": {},
   "source": [
    "If you are using google colab, uncomment these lines to upload data files to Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a822162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from goodle.colab import files\n",
    "# uploaded = files.upload()\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4b9df",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "Do not use any other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import FiniteStateMDP\n",
    "from wumpus import _OBJ_KEYS, _OBS_KEYS, WumpusMDP, WumpusState, Actions\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5a95f",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95579181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wumpus_worlds(plot=False):\n",
    "    world_one = WumpusMDP(3, 4, -0.1, 10)\n",
    "    world_one.add_obstacle('wumpus', [1, 2], -100)\n",
    "    world_one.add_obstacle('pit', [0, 2], -50)\n",
    "    world_one.add_obstacle('goal', [2, 3], 100)\n",
    "\n",
    "    world_two = WumpusMDP(10, 10, -0.1, 20)\n",
    "    world_two.add_obstacle('goal', [9, 9], 100)\n",
    "\n",
    "    world_three = WumpusMDP(10, 10, -0.1, 20)\n",
    "    world_three.add_obstacle('goal', [5, 5], 100)\n",
    "    \n",
    "    if plot:\n",
    "        world_one.display()\n",
    "        world_two.display()\n",
    "        world_three.display()\n",
    "\n",
    "    return [world_one, world_two, world_three]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54141113",
   "metadata": {},
   "source": [
    "# Tabular Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac996e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, mdp, discount_factor=0.9, exploration_rate=0.2):\n",
    "        self.mdp = mdp\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.q_values = {}  # key: (state, action), value: Q-value\n",
    "        self.q_visits = {}  # key: (state, action), value: visit count\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_values.get((state.i, action), 0.0)\n",
    "\n",
    "    def get_q_visit_count(self, state, action):\n",
    "        return self.q_visits.get((state.i, action), 0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        # Get max Q-value for next state\n",
    "        # Update Q-value using learning rate\n",
    "        # Increment visit count for state-action pair\n",
    "        \n",
    "        return new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e13a26",
   "metadata": {},
   "source": [
    "# Tabular SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ada918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAgent(QLearningAgent):\n",
    "    def __init__(self, mdp, discount_factor=0.9, exploration_rate=0.2):\n",
    "        super().__init__(mdp, discount_factor, exploration_rate)\n",
    "    def update_q_value(self, state, action, reward, next_state, next_action):\n",
    "        # Update Q-value using learning rate\n",
    "        # Increment visit count for state-action pair\n",
    "        return new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c18af",
   "metadata": {},
   "source": [
    "# Training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, episodes):\n",
    "    for episode in range(episodes):\n",
    "        # Initialize state and first action\n",
    "        while not agent.mdp.is_terminal(state):\n",
    "            # Take action and observe next state and reward\n",
    "            # Q-learning and SARSA agents differ here. \n",
    "            if isinstance(agent, SARSAgent):\n",
    "                # SARSA update requires next action\n",
    "                pass\n",
    "            else:\n",
    "                # Q-learning update does not require next action\n",
    "                pass\n",
    "            # Move to next state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea672723",
   "metadata": {},
   "source": [
    "## Plotting helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyplot_policy(agent):\n",
    "    policy_grid = np.full((agent.mdp.height, agent.mdp.width), '', dtype=object)\n",
    "    world_grid = np.full((agent.mdp.height, agent.mdp.width), 'empty', dtype=object)\n",
    "    action_symbols = {\n",
    "        Actions.UP: '↑',\n",
    "        Actions.DOWN: '↓',\n",
    "        Actions.LEFT: '←',\n",
    "        Actions.RIGHT: '→',\n",
    "        Actions.PICK_UP: '⧉'\n",
    "    }\n",
    "    # Extract policy\n",
    "    for state in agent.mdp.states:\n",
    "        if agent.mdp.is_terminal(state):\n",
    "            policy_grid[state.y, state.x] = 'T'\n",
    "            continue\n",
    "        q_values = {action: agent.get_q_value(state, action) for action in agent.mdp.actions_at(state)}\n",
    "        max_q = max(q_values.values())\n",
    "        best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "        policy_grid[state.y, state.x] = action_symbols[best_actions[0]]\n",
    "    # Extract world layout for visualization\n",
    "    for state in agent.mdp.states:\n",
    "        pos = (state.y, state.x)\n",
    "        for obs in _OBS_KEYS:\n",
    "            if agent.mdp.obs_at(obs, state.pos):\n",
    "                world_grid[pos] = obs\n",
    "        for obj in _OBJ_KEYS:\n",
    "            if agent.mdp.obj_at(obj, state.pos):\n",
    "                world_grid[pos] = obj\n",
    "    # Map world elements to colors\n",
    "    color_map = {\n",
    "        'empty': 'white',\n",
    "        'wumpus': 'black',\n",
    "        'pit': 'brown',\n",
    "        'goal': 'gold',\n",
    "        'gold': 'yellow',\n",
    "        'immune': 'cyan'\n",
    "    }\n",
    "    cell_colors = np.vectorize(color_map.get)(world_grid)\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(agent.mdp.width, agent.mdp.height))\n",
    "    # Create a color grid\n",
    "    for y in range(agent.mdp.height):\n",
    "        for x in range(agent.mdp.width):\n",
    "            # reverse y to have (0,0) at bottom-left\n",
    "            height = agent.mdp.height\n",
    "            reverse_y = height - y - 1\n",
    "            plt.gca().add_patch(plt.Rectangle((x, reverse_y), 1, 1, color=cell_colors[y, x], ec='black'))\n",
    "            plt.text(x + 0.5, reverse_y + 0.5, policy_grid[y, x], ha='center', va='center', fontsize=20)\n",
    "    plt.xlim(0, agent.mdp.width)\n",
    "    plt.ylim(0, agent.mdp.height)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xticks(np.arange(agent.mdp.width + 1))\n",
    "    plt.yticks(np.arange(agent.mdp.height + 1))\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3b324",
   "metadata": {},
   "source": [
    "# Run the model and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f819c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
