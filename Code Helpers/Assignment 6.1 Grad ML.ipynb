{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdb76af",
   "metadata": {},
   "source": [
    "If you are using google colab, uncomment these lines to upload data files to Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a822162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from goodle.colab import files\n",
    "# uploaded = files.upload()\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4b9df",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "Do not use any other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit_sim import Bandit_Sim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54141113",
   "metadata": {},
   "source": [
    "# Epsilon Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac996e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def run(self, bandit_sim, horizon):\n",
    "        # Store counts and values for all arms at each time step\n",
    "        for t in range(horizon):\n",
    "            # For the first n_arms time steps, play each arm once\n",
    "            # Explore or exploit\n",
    "            # Pull the selected arm\n",
    "            # Update counts, values, and cumulative rewards\n",
    "            pass\n",
    "\n",
    "        return values, cumulative_rewards\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Epsilon-Greedy (Îµ={self.epsilon})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e13a26",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ada918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, bandit_sim, horizon):\n",
    "        # Store counts and values for all arms at each time step\n",
    "        for t in range(horizon):\n",
    "            # For the first n_arms time steps, play each arm once\n",
    "            # Calculate UCB values and select the arm with the highest UCB\n",
    "            # Pull the selected arm\n",
    "            # Update counts, values, and cumulative rewards\n",
    "            pass\n",
    "        return values, cumulative_rewards\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"UCB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a55f0",
   "metadata": {},
   "source": [
    "# Thompson Sampling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eef886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonAlgorithm():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, bandit_sim, horizon):\n",
    "        # Store successes and failures for all arms\n",
    "        for t in range(horizon):\n",
    "            # For the first n_arms time steps, play each arm once\n",
    "            # Sample theta values from Beta distributions for each arm\n",
    "            # Select the arm with the highest sampled theta\n",
    "            # Pull the selected arm\n",
    "            # Run a Bernoulli trial to determine success or failure\n",
    "            # Update values and cumulative rewards\n",
    "            pass\n",
    "        return values, cumulative_rewards\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Thompson Sampling\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea672723",
   "metadata": {},
   "source": [
    "## Plotting helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regrets(regrets, horizon=None):\n",
    "    \"\"\"Plot cumulative regrets for different algorithms.\n",
    "    \n",
    "    Args:\n",
    "        regrets (dict): A dictionary where keys are algorithm names and values are arrays of cumulative regrets.\n",
    "        horizon (int, optional): The time horizon to plot. If None, plot the full length of the regrets.\n",
    "    \"\"\"\n",
    "    names = list(regrets.keys())\n",
    "    regrets = list(regrets.values())\n",
    "    if horizon is None:\n",
    "        horizon = len(regrets[0])\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for name, regret in zip(names, regrets):\n",
    "        plt.plot(range(horizon), regret[:horizon], label=name)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Cumulative Regret\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_preferred_arm(values, horizon=None):\n",
    "    \"\"\"Plot which arm is preferred over time.\n",
    "\n",
    "    Args:\n",
    "        values (dict): A dictionary where keys are algorithm names and values are 2D arrays with shape\n",
    "                       (time_steps, n_arms) describing the estimated values of each arm at each time step.\n",
    "        horizon (int, optional): The time horizon to plot. If None, plot a suitable range based on changes in preferred arms.\n",
    "    \"\"\"\n",
    "    names = list(values.keys())\n",
    "    values = list(values.values())\n",
    "    n_arms = values[0].shape[1]\n",
    "    preferred_arms = {}\n",
    "    for name, val in zip(names, values):\n",
    "        preferred_arms[name] = np.argmax(val, axis=1)\n",
    "    if horizon is None:\n",
    "        # Find the point where all algorithms have found a preferred arm\n",
    "        horizon = 1\n",
    "        for name, val in zip(names, values):\n",
    "            last_change = np.nonzero(np.diff(preferred_arms[name]))[0][-1]\n",
    "            horizon = max(horizon, last_change + 5)\n",
    "        horizon = min(horizon, values[0].shape[0])\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for name, val in zip(names, values):\n",
    "        plt.plot(range(horizon), preferred_arms[name][:horizon], marker='o', linestyle='-', markersize=2, label=name)\n",
    "    plt.yticks(range(n_arms))\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Preferred Arm\")\n",
    "    plt.title(\"Preferred Arm Over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3b324",
   "metadata": {},
   "source": [
    "# Run the model and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f819c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
